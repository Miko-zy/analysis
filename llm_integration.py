import requests
import json
import re
import time
import logging
import traceback
from typing import Dict, List, Any, Tuple, Optional
from config import OLLAMA_CONFIG
from sql_validator import SQLValidator

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SQLTemplateSystem:
    """SQLæ¨¡æ¿ç³»ç»Ÿ - å†…åµŒç‰ˆæœ¬"""

    TEMPLATES = {
        'basic_select': {
            'description': 'åŸºç¡€æŸ¥è¯¢',
            'sql_template': "SELECT {columns} FROM `{table}` {where} {group_by} {order_by} {limit}",
            'default_limit': 100
        },
        'summary_stats': {
            'description': 'ç»Ÿè®¡æ±‡æ€»',
            'sql_template': """
SELECT 
    COUNT(*) as æ€»è®°å½•æ•°,
    {numeric_columns}
FROM `{table}`
{where}
            """,
            'numeric_columns_template': "AVG(`{column}`) as `{column}_å¹³å‡å€¼`, MAX(`{column}`) as `{column}_æœ€å¤§å€¼`, MIN(`{column}`) as `{column}_æœ€å°å€¼`"
        },
        'group_by_stats': {
            'description': 'åˆ†ç»„ç»Ÿè®¡',
            'sql_template': """
SELECT 
    `{group_column}` as åˆ†ç»„,
    COUNT(*) as è®°å½•æ•°,
    {agg_columns}
FROM `{table}`
{where}
GROUP BY `{group_column}`
ORDER BY è®°å½•æ•° DESC
{limit}
            """
        },
        'time_series': {
            'description': 'æ—¶é—´åºåˆ—åˆ†æ',
            'sql_template': """
SELECT 
    DATE(`{date_column}`) as æ—¥æœŸ,
    COUNT(*) as è®°å½•æ•°,
    {agg_columns}
FROM `{table}`
WHERE `{date_column}` IS NOT NULL
GROUP BY DATE(`{date_column}`)
ORDER BY æ—¥æœŸ
{limit}
            """
        },
        'ranking': {
            'description': 'æ’åæŸ¥è¯¢',
            'sql_template': """
SELECT 
    `{ranking_column}` as åç§°,
    `{value_column}` as æ•°å€¼
FROM `{table}`
WHERE `{value_column}` IS NOT NULL
ORDER BY `{value_column}` DESC
{limit}
            """
        },
        'related_query': {
            'description': 'å…³è”æŸ¥è¯¢',
            'sql_template': """
SELECT 
    t1.`{t1_column}` as è¡¨1å­—æ®µ,
    t2.`{t2_column}` as è¡¨2å­—æ®µ,
    COUNT(*) as å…³è”æ•°
FROM `{table1}` t1
JOIN `{table2}` t2 ON t1.`{join_key1}` = t2.`{join_key2}`
{where}
GROUP BY t1.`{t1_column}`, t2.`{t2_column}`
ORDER BY å…³è”æ•° DESC
{limit}
            """
        }
    }

    @staticmethod
    def classify_query_intent(natural_language_query: str) -> Tuple[str, Dict[str, Any]]:
        """åˆ†ç±»æŸ¥è¯¢æ„å›¾"""
        query_lower = natural_language_query.lower()

        keyword_patterns = {
            'summary_stats': ['æ€»è®¡', 'åˆè®¡', 'æ±‡æ€»', 'æ€»å’Œ', 'å¹³å‡', 'æœ€å¤§å€¼', 'æœ€å°å€¼', 'ç»Ÿè®¡', 'æ•°é‡'],
            'group_by_stats': ['åˆ†ç»„', 'åˆ†ç±»', 'æŒ‰.*ç»Ÿè®¡', 'å„ä¸ª.*çš„', 'æ¯ç§', 'æ¯ç±»', 'å„åœ°åŒº'],
            'time_series': ['æ—¶é—´', 'æ—¥æœŸ', 'æœˆä»½', 'å­£åº¦', 'å¹´ä»½', 'è¶‹åŠ¿', 'æ¯å¤©', 'æ¯æœˆ', 'é€å¹´'],
            'ranking': ['æ’å', 'å‰.*å', 'æœ€é«˜', 'æœ€ä½', 'æœ€å¤š', 'æœ€å°‘', 'æœ€å¥½', 'æœ€å·®', 'top'],
            'related_query': ['å…³è”', 'å…³ç³»', 'è¿æ¥', 'æ¶‰åŠ', 'å’Œ.*ä¸€èµ·', 'åŒæ—¶'],
            'basic_select': ['æŸ¥è¯¢', 'æŸ¥çœ‹', 'æ˜¾ç¤º', 'åˆ—å‡º', 'æ‰¾', 'æœç´¢']
        }

        matched_intent = 'basic_select'
        confidence = 0.0

        for intent, patterns in keyword_patterns.items():
            for pattern in patterns:
                if re.search(pattern, query_lower):
                    if intent != matched_intent:
                        matched_intent = intent
                        confidence = 0.8
                        break

        extracted_params = SQLTemplateSystem._extract_query_params(natural_language_query)

        return matched_intent, {
            'intent': matched_intent,
            'confidence': confidence,
            'extracted_params': extracted_params
        }

    @staticmethod
    def _extract_query_params(query: str) -> Dict[str, Any]:
        """ä»è‡ªç„¶è¯­è¨€æŸ¥è¯¢ä¸­æå–å‚æ•°"""
        params = {
            'table_names': [],
            'column_names': [],
            'numeric_columns': [],
            'date_columns': [],
            'text_columns': [],
            'filters': [],
            'sort_order': 'DESC',
            'limit_value': 10
        }

        table_patterns = ['è¡¨\s*[ï¼š"\']?([^"\'ï¼Œ,ã€‚\.\s]+)']
        for pattern in table_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            if matches:
                params['table_names'].extend(matches)

        column_patterns = ['å­—æ®µ\s*[ï¼š"\']?([^"\'ï¼Œ,ã€‚\.\s]+)', 'åˆ—\s*[ï¼š"\']?([^"\'ï¼Œ,ã€‚\.\s]+)']
        for pattern in column_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            if matches:
                params['column_names'].extend(matches)

        limit_matches = re.findall(r'å‰\s*(\d+)\s*å', query)
        if limit_matches:
            params['limit_value'] = int(limit_matches[0])
        elif re.search(r'å‰å|å‰20|å‰50', query):
            num_match = re.search(r'å‰(\d+)', query)
            if num_match:
                params['limit_value'] = int(num_match.group(1))

        if re.search(r'æœ€ä½|æœ€å°‘|æœ€å°|é™åº|å€’åº', query):
            params['sort_order'] = 'ASC'

        return params

    @staticmethod
    def generate_from_template(
            intent: str,
            table_info: Dict[str, List[Dict]],
            extracted_params: Dict[str, Any]
    ) -> str:
        """æ ¹æ®æ¨¡æ¿ç”ŸæˆSQL"""
        if intent not in SQLTemplateSystem.TEMPLATES:
            intent = 'basic_select'

        template = SQLTemplateSystem.TEMPLATES[intent]
        sql_template = template['sql_template']

        tables = list(table_info.keys())
        if not tables:
            return "SELECT 1"

        selected_table = tables[0]
        columns = table_info[selected_table]

        numeric_cols = [col['name'] for col in columns if col.get('category') in ['integer', 'numeric']]
        text_cols = [col['name'] for col in columns if col.get('category') == 'text']
        date_cols = [col['name'] for col in columns if col.get('category') == 'datetime']

        template_params = {
            'table': selected_table,
            'limit': f"LIMIT {extracted_params.get('limit_value', 100)}"
        }

        if intent == 'basic_select':
            select_cols = []
            for col in columns[:5]:
                col_name = col['name']
                alias = SQLTemplateSystem._get_column_alias(col_name)
                select_cols.append(f"`{col_name}` as `{alias}`")

            template_params['columns'] = ', '.join(select_cols) if select_cols else '*'
            template_params['where'] = ''
            template_params['group_by'] = ''
            template_params['order_by'] = 'ORDER BY 1'

        elif intent == 'summary_stats':
            if numeric_cols:
                numeric_templates = []
                for col in numeric_cols[:3]:
                    template_str = template['numeric_columns_template']
                    numeric_templates.append(template_str.format(column=col))

                template_params['numeric_columns'] = ', '.join(numeric_templates)
            else:
                template_params['numeric_columns'] = 'NULL as æ— æ•°å€¼åˆ—'

        elif intent == 'group_by_stats':
            if text_cols:
                template_params['group_column'] = text_cols[0]
            elif columns:
                template_params['group_column'] = columns[0]['name']
            else:
                template_params['group_column'] = 'id'

            if numeric_cols:
                agg_columns = []
                for col in numeric_cols[:2]:
                    agg_columns.append(f"SUM(`{col}`) as `{col}_æ€»å’Œ`, AVG(`{col}`) as `{col}_å¹³å‡`")
                template_params['agg_columns'] = ', '.join(agg_columns)
            else:
                template_params['agg_columns'] = 'NULL as æ— æ•°å€¼åˆ—'

        elif intent == 'time_series':
            if date_cols:
                template_params['date_column'] = date_cols[0]
            else:
                return SQLTemplateSystem.generate_from_template('group_by_stats', table_info, extracted_params)

            if numeric_cols:
                agg_columns = []
                for col in numeric_cols[:2]:
                    agg_columns.append(f"SUM(`{col}`) as `{col}_æ€»è®¡`")
                template_params['agg_columns'] = ', '.join(agg_columns)
            else:
                template_params['agg_columns'] = 'COUNT(*) as è®°å½•æ•°'

        elif intent == 'ranking':
            if numeric_cols and text_cols:
                template_params['ranking_column'] = text_cols[0]
                template_params['value_column'] = numeric_cols[0]
            elif numeric_cols and columns:
                template_params['ranking_column'] = columns[0]['name']
                template_params['value_column'] = numeric_cols[0]
            else:
                return SQLTemplateSystem.generate_from_template('basic_select', table_info, extracted_params)

        elif intent == 'related_query':
            if len(tables) >= 2:
                template_params['table1'] = tables[0]
                template_params['table2'] = tables[1]

                table1_cols = [col['name'] for col in table_info[tables[0]]]
                table2_cols = [col['name'] for col in table_info[tables[1]]]

                join_keys = set(table1_cols).intersection(set(table2_cols))
                if join_keys:
                    join_key = list(join_keys)[0]
                    template_params['join_key1'] = join_key
                    template_params['join_key2'] = join_key
                else:
                    template_params['join_key1'] = table1_cols[0] if table1_cols else 'id'
                    template_params['join_key2'] = table2_cols[0] if table2_cols else 'id'

                template_params['t1_column'] = table1_cols[0] if table1_cols else 'id'
                template_params['t2_column'] = table2_cols[0] if table2_cols else 'id'
            else:
                return SQLTemplateSystem.generate_from_template('group_by_stats', table_info, extracted_params)

        try:
            sql = sql_template.format(**template_params)
            sql = re.sub(r'\n\s*\n', '\n', sql)
            return sql.strip()
        except Exception as e:
            return f"SELECT * FROM `{selected_table}` LIMIT 10"

    @staticmethod
    def _get_column_alias(column_name: str) -> str:
        """è·å–åˆ—çš„ä¸­æ–‡åˆ«å"""
        col_lower = column_name.lower()

        english_mappings = {
            'id': 'ID', 'name': 'åç§°', 'title': 'æ ‡é¢˜', 'desc': 'æè¿°', 'description': 'æè¿°',
            'date': 'æ—¥æœŸ', 'time': 'æ—¶é—´', 'datetime': 'æ—¥æœŸæ—¶é—´', 'create_time': 'åˆ›å»ºæ—¶é—´',
            'update_time': 'æ›´æ–°æ—¶é—´', 'amount': 'é‡‘é¢', 'price': 'ä»·æ ¼', 'cost': 'æˆæœ¬',
            'fee': 'è´¹ç”¨', 'money': 'é‡‘é¢', 'total': 'æ€»è®¡', 'sum': 'åˆè®¡', 'count': 'æ•°é‡',
            'quantity': 'æ•°é‡', 'qty': 'æ•°é‡', 'number': 'ç¼–å·', 'num': 'ç¼–å·', 'status': 'çŠ¶æ€',
            'state': 'çŠ¶æ€', 'type': 'ç±»å‹', 'category': 'åˆ†ç±»', 'class': 'ç±»åˆ«', 'group': 'åˆ†ç»„',
            'user': 'ç”¨æˆ·', 'username': 'ç”¨æˆ·å', 'password': 'å¯†ç ', 'email': 'é‚®ç®±',
            'phone': 'ç”µè¯', 'mobile': 'æ‰‹æœº', 'address': 'åœ°å€', 'city': 'åŸå¸‚',
            'province': 'çœä»½', 'country': 'å›½å®¶', 'region': 'åŒºåŸŸ', 'area': 'åœ°åŒº',
            'score': 'åˆ†æ•°', 'grade': 'ç­‰çº§', 'level': 'çº§åˆ«', 'rate': 'æ¯”ç‡', 'ratio': 'æ¯”ä¾‹',
            'percent': 'ç™¾åˆ†æ¯”', 'percentage': 'ç™¾åˆ†æ¯”', 'age': 'å¹´é¾„', 'gender': 'æ€§åˆ«',
            'sex': 'æ€§åˆ«', 'birth': 'ç”Ÿæ—¥', 'birthday': 'ç”Ÿæ—¥'
        }

        if re.search(r'[\u4e00-\u9fff]', column_name):
            return column_name

        for eng_key, chi_value in english_mappings.items():
            if eng_key == col_lower or f'_{eng_key}' in col_lower or f'{eng_key}_' in col_lower:
                return chi_value

        suffixes = {
            '_id': 'ID', '_name': 'åç§°', '_time': 'æ—¶é—´', '_date': 'æ—¥æœŸ',
            '_amount': 'é‡‘é¢', '_price': 'ä»·æ ¼', '_count': 'æ•°é‡', '_total': 'æ€»è®¡',
            '_status': 'çŠ¶æ€', '_type': 'ç±»å‹'
        }

        for suffix, alias in suffixes.items():
            if col_lower.endswith(suffix):
                prefix = col_lower[:-len(suffix)]
                if prefix in english_mappings:
                    return f"{english_mappings[prefix]}{alias}"
                else:
                    return f"{prefix}{alias}"

        return column_name


class LLMAnalyst:
    def __init__(self):
        self.base_url = OLLAMA_CONFIG['base_url']
        self.model = OLLAMA_CONFIG['model']
        self.timeout = OLLAMA_CONFIG['timeout']
        self.max_retries = 3
        self.retry_delay = 2
        self.template_system = SQLTemplateSystem()
        self.sql_validator = SQLValidator()
        self.use_template_first = True
        self.auto_fix_columns = True

    def _call_ollama(self, prompt, system_prompt=None, temperature=0.1):
        """è°ƒç”¨Ollama API"""
        for attempt in range(self.max_retries):
            try:
                messages = []
                if system_prompt:
                    messages.append({"role": "system", "content": system_prompt})
                messages.append({"role": "user", "content": prompt})

                payload = {
                    "model": self.model,
                    "messages": messages,
                    "stream": False,
                    "options": {
                        "temperature": temperature,
                        "top_p": 0.9,
                        "top_k": 40,
                        "num_predict": 6000
                    }
                }

                response = requests.post(
                    f"{self.base_url}/api/chat",
                    json=payload,
                    timeout=self.timeout
                )

                if response.status_code != 200:
                    error_msg = f"Ollama APIé”™è¯¯: çŠ¶æ€ç  {response.status_code}, å“åº”: {response.text}"
                    logger.error(error_msg)
                    if attempt == self.max_retries - 1:
                        return error_msg
                    time.sleep(self.retry_delay)
                    continue

                result = response.json()

                if 'message' in result and 'content' in result['message']:
                    content = result['message']['content'].strip()
                    logger.info(f"Ollamaå“åº”æˆåŠŸï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
                    return content
                else:
                    error_msg = f"å“åº”æ ¼å¼é”™è¯¯: {result}"
                    logger.error(error_msg)
                    return error_msg

            except requests.exceptions.ConnectionError:
                error_msg = f"æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡ï¼Œè¯·ç¡®ä¿Ollamaæ­£åœ¨è¿è¡Œåœ¨ {self.base_url}"
                logger.error(error_msg)
                if attempt == self.max_retries - 1:
                    return error_msg
                time.sleep(self.retry_delay)

            except requests.exceptions.Timeout:
                error_msg = "è¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€"
                logger.error(error_msg)
                if attempt == self.max_retries - 1:
                    return error_msg
                time.sleep(self.retry_delay)

            except Exception as e:
                error_msg = f"æœªçŸ¥é”™è¯¯: {str(e)}"
                logger.error(error_msg)
                return error_msg

        return "æ‰€æœ‰é‡è¯•å°è¯•éƒ½å¤±è´¥äº†"

    def _build_detailed_schema_info(self, table_schemas: Dict[str, List[Dict]]) -> str:
        """æ„å»ºè¯¦ç»†çš„è¡¨ç»“æ„ä¿¡æ¯"""
        schema_info = ""

        for table_name, columns in table_schemas.items():
            primary_keys = [col['name'] for col in columns if col.get('primary_key', False)]
            numeric_cols = [col['name'] for col in columns if col.get('category') in ['integer', 'numeric']]
            text_cols = [col['name'] for col in columns if col.get('category') == 'text']
            date_cols = [col['name'] for col in columns if col.get('category') == 'datetime']

            schema_info += f"ã€è¡¨: {table_name}ã€‘\n"

            if primary_keys:
                schema_info += f"  ä¸»é”®: {', '.join(primary_keys)}\n"

            schema_info += f"  å­—æ®µåˆ—è¡¨ ({len(columns)} ä¸ª):\n"

            for i, col in enumerate(columns[:10], 1):
                col_type = str(col.get('type', '')).upper()
                nullable = "å¯ç©º" if col.get('nullable', True) else "éç©º"
                pk = "ä¸»é”®" if col.get('primary_key', False) else ""

                col_name = col['name'].lower()
                inferred_type = ""

                if any(x in col_name for x in ['id', 'code', 'no', 'num']):
                    inferred_type = "æ ‡è¯†ç¬¦"
                elif any(x in col_name for x in ['name', 'title', 'desc', 'note']):
                    inferred_type = "åç§°/æè¿°"
                elif any(x in col_name for x in ['date', 'time', 'year', 'month', 'day']):
                    inferred_type = "æ—¥æœŸæ—¶é—´"
                elif any(x in col_name for x in ['amount', 'price', 'cost', 'fee', 'money']):
                    inferred_type = "é‡‘é¢"
                elif any(x in col_name for x in ['count', 'quantity', 'qty', 'number']):
                    inferred_type = "æ•°é‡"
                elif any(x in col_name for x in ['rate', 'ratio', 'percent']):
                    inferred_type = "æ¯”ç‡"
                elif any(x in col_name for x in ['status', 'type', 'category', 'class']):
                    inferred_type = "åˆ†ç±»"
                elif any(x in col_name for x in ['phone', 'email', 'address']):
                    inferred_type = "è”ç³»æ–¹å¼"

                if inferred_type:
                    inferred_type = f" ({inferred_type})"

                schema_info += f"  {i}. `{col['name']}` {col_type} {nullable} {pk}{inferred_type}\n"

            schema_info += f"  ç»Ÿè®¡: {len(numeric_cols)}ä¸ªæ•°å€¼å­—æ®µ, {len(text_cols)}ä¸ªæ–‡æœ¬å­—æ®µ, {len(date_cols)}ä¸ªæ—¥æœŸå­—æ®µ\n\n"

        return schema_info

    def generate_sql_query(self, natural_language_query: str, table_schemas: Dict[str, List[Dict]]) -> str:
        """æ ¹æ®è‡ªç„¶è¯­è¨€ç”ŸæˆSQLæŸ¥è¯¢"""
        if not natural_language_query.strip():
            return "è¯·è¾“å…¥æŸ¥è¯¢é—®é¢˜"

        try:
            logger.info(f"ä¸ºè‡ªç„¶è¯­è¨€æŸ¥è¯¢ç”ŸæˆSQL: {natural_language_query}")

            # é˜¶æ®µ1ï¼šä½¿ç”¨æ¨¡æ¿ç³»ç»Ÿç”ŸæˆSQL
            template_sql = self._generate_sql_from_template(natural_language_query, table_schemas)

            # éªŒè¯æ¨¡æ¿SQL
            if self.auto_fix_columns:
                template_sql = self._validate_and_fix_sql(template_sql, table_schemas)

            # é˜¶æ®µ2ï¼šå¦‚æœé…ç½®äº†Ollamaä¸”è¿æ¥æ­£å¸¸ï¼Œå°è¯•ä¼˜åŒ–
            if self.base_url and self.model and self.base_url != "":
                try:
                    # ä½¿ç”¨å¤§æ¨¡å‹ä¼˜åŒ–æ¨¡æ¿SQL
                    optimized_sql = self._optimize_sql_with_llm(
                        natural_language_query,
                        table_schemas,
                        template_sql
                    )

                    # éªŒè¯å¹¶ä¿®æ­£ä¼˜åŒ–åçš„SQL
                    if self.auto_fix_columns:
                        optimized_sql = self._validate_and_fix_sql(optimized_sql, table_schemas)

                    # éªŒè¯ä¼˜åŒ–åçš„SQL
                    if self._validate_sql_quality(optimized_sql, template_sql):
                        logger.info("ä½¿ç”¨å¤§æ¨¡å‹ä¼˜åŒ–çš„SQL")
                        return optimized_sql
                    else:
                        logger.warning("å¤§æ¨¡å‹ä¼˜åŒ–çš„SQLè´¨é‡ä¸ä½³ï¼Œä½¿ç”¨æ¨¡æ¿SQL")
                        return template_sql

                except Exception as e:
                    logger.warning(f"å¤§æ¨¡å‹ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ¿SQL: {e}")
                    return template_sql
            else:
                # æ²¡æœ‰é…ç½®å¤§æ¨¡å‹ï¼Œç›´æ¥ä½¿ç”¨æ¨¡æ¿SQL
                logger.info("ä½¿ç”¨æ¨¡æ¿ç³»ç»Ÿç”Ÿæˆçš„SQL")
                return template_sql

        except Exception as e:
            error_msg = f"ç”ŸæˆSQLæ—¶å‡ºç°å¼‚å¸¸: {str(e)}"
            logger.error(error_msg)
            logger.error(traceback.format_exc())
            # è¿”å›å®‰å…¨çš„æŸ¥è¯¢
            return self._generate_safe_fallback_sql(table_schemas)

    def _generate_sql_from_template(self, natural_language_query: str, table_schemas: Dict) -> str:
        """ä½¿ç”¨æ¨¡æ¿ç³»ç»Ÿç”ŸæˆSQL"""
        try:
            intent, intent_info = self.template_system.classify_query_intent(natural_language_query)
            logger.info(f"æŸ¥è¯¢æ„å›¾åˆ†ç±»: {intent}, ç½®ä¿¡åº¦: {intent_info['confidence']}")

            template_sql = self.template_system.generate_from_template(
                intent,
                table_schemas,
                intent_info['extracted_params']
            )

            template_desc = self.template_system.TEMPLATES.get(intent, {}).get('description', 'æŸ¥è¯¢')
            commented_sql = f"-- åŸºäºæ¨¡æ¿ç”Ÿæˆ: {template_desc}\n{template_sql}"

            logger.info(f"æ¨¡æ¿ç”ŸæˆSQLæˆåŠŸï¼Œæ„å›¾: {intent}")
            return commented_sql

        except Exception as e:
            logger.error(f"æ¨¡æ¿ç³»ç»Ÿç”ŸæˆSQLå¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            tables = list(table_schemas.keys())
            if tables:
                return f"SELECT * FROM `{tables[0]}` LIMIT 10"
            return "SELECT 1"

    def _validate_and_fix_sql(self, sql: str, table_schemas: Dict[str, List[Dict]]) -> str:
        """éªŒè¯å¹¶ä¿®æ­£SQL"""
        try:
            # 1. éªŒè¯SQLç»“æ„
            is_valid, warnings = self.sql_validator.validate_sql_structure(sql)
            if warnings:
                logger.warning(f"SQLç»“æ„è­¦å‘Š: {warnings}")

            # 2. æå–è¡¨å
            tables = self.sql_validator.extract_tables_from_sql(sql)

            if not tables:
                logger.warning("SQLä¸­æ²¡æœ‰æ‰¾åˆ°è¡¨åï¼Œç”Ÿæˆå®‰å…¨æŸ¥è¯¢")
                return self._generate_safe_fallback_sql(table_schemas)

            # 3. ä¸ºæ¯ä¸ªè¡¨ä¿®æ­£åˆ—å
            for table_name in tables:
                if table_name in table_schemas:
                    available_columns = [col['name'] for col in table_schemas[table_name]]
                    # ä¿®æ­£åˆ—å
                    fixed_sql, corrections = self.sql_validator.fix_column_names(sql, available_columns, table_name)

                    if corrections:
                        logger.info(f"ä¿®æ­£äº†åˆ—å: {corrections}")
                        sql = fixed_sql
                else:
                    logger.warning(f"è¡¨ '{table_name}' ä¸åœ¨æ•°æ®åº“è¡¨ç»“æ„ä¸­")

            # 4. ç¡®ä¿æœ‰LIMIT
            if 'LIMIT' not in sql.upper() and 'SELECT' in sql.upper():
                if 'ORDER BY' in sql.upper():
                    sql = re.sub(
                        r'(ORDER BY.*?)(?=$|;)',
                        r'\1 LIMIT 50',
                        sql,
                        flags=re.IGNORECASE | re.DOTALL
                    )
                else:
                    sql += ' LIMIT 50'

            return sql

        except Exception as e:
            logger.error(f"éªŒè¯ä¿®æ­£SQLå¤±è´¥: {e}")
            return sql

    def _generate_safe_fallback_sql(self, table_schemas: Dict[str, List[Dict]]) -> str:
        """ç”Ÿæˆå®‰å…¨çš„å›é€€SQL"""
        tables = list(table_schemas.keys())
        if not tables:
            return "SELECT 1"

        table_name = tables[0]
        schema = table_schemas[table_name]

        return self.sql_validator.generate_safe_sql(table_name, schema)

    def _optimize_sql_with_llm(self, natural_language_query: str, table_schemas: Dict, template_sql: str) -> str:
        """ä½¿ç”¨å¤§æ¨¡å‹ä¼˜åŒ–SQL"""
        # æ„å»ºè¯¦ç»†çš„schemaä¿¡æ¯
        schema_info = self._build_detailed_schema_info(table_schemas)

        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªSQLä¸“å®¶ã€‚è¯·æ ¹æ®ç”¨æˆ·éœ€æ±‚ä¼˜åŒ–ä»¥ä¸‹SQLæŸ¥è¯¢ã€‚

ç”¨æˆ·éœ€æ±‚: {natural_language_query}

æ•°æ®åº“è¡¨ç»“æ„:
{schema_info}

ç°æœ‰SQLæ¨¡æ¿ï¼ˆä»…ä¾›å‚è€ƒï¼‰:
{template_sql}

è¯·ä¼˜åŒ–è¿™ä¸ªSQLæŸ¥è¯¢ï¼Œç¡®ä¿ï¼š
1. å‡†ç¡®åæ˜ ç”¨æˆ·éœ€æ±‚
2. è¯­æ³•æ­£ç¡®
3. ä½¿ç”¨æ­£ç¡®çš„è¡¨åå’Œåˆ—åï¼ˆç”¨åå¼•å·åŒ…è£¹ï¼‰
4. åŒ…å«é€‚å½“çš„LIMITå­å¥
5. æ·»åŠ æœ‰æ„ä¹‰çš„åˆ—åˆ«å
6. ä½¿ç”¨çš„è¡¨åå’Œå­—ç¬¦ä¸²å€¼å¿…é¡»æ¥æºäºæ•°æ®åº“

è¯·ç›´æ¥è¿”å›ä¼˜åŒ–åçš„SQLè¯­å¥ï¼ˆåªè¿”å›SQLï¼Œä¸è¦æœ‰å…¶ä»–å†…å®¹ï¼‰:
"""

        system_prompt = """ä½ æ˜¯ä¸€ä¸ªSQLä¼˜åŒ–ä¸“å®¶ã€‚è¯·ç›´æ¥è¿”å›ä¼˜åŒ–åçš„SQLè¯­å¥ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šã€æ³¨é‡Šæˆ–å…¶ä»–æ–‡æœ¬ã€‚"""

        sql_query = self._call_ollama(prompt, system_prompt, temperature=0.1)

        # æ¸…ç†å“åº”
        sql_query = sql_query.strip()
        sql_query = re.sub(r'```(?:sql)?|```', '', sql_query).strip()

        # æå–SQLè¯­å¥
        sql_match = re.search(r'(SELECT\s+.*?)(?=;|$)', sql_query, re.IGNORECASE | re.DOTALL)
        if sql_match:
            sql_query = sql_match.group(1).strip()

        # ç¡®ä¿æœ‰SELECT
        if not sql_query.upper().startswith('SELECT'):
            return template_sql

        return sql_query

    def _validate_sql_quality(self, llm_sql: str, template_sql: str) -> bool:
        """éªŒè¯SQLè´¨é‡"""
        # åŸºæœ¬æ£€æŸ¥
        if not llm_sql or len(llm_sql.strip()) < 10:
            return False

        # å¿…é¡»åŒ…å«SELECT
        if 'SELECT' not in llm_sql.upper():
            return False

        # å¿…é¡»åŒ…å«FROM
        if 'FROM' not in llm_sql.upper():
            return False

        # æ£€æŸ¥å±é™©æ“ä½œ
        dangerous = ['DROP', 'DELETE', 'UPDATE', 'INSERT', 'ALTER', 'CREATE', 'TRUNCATE']
        for d in dangerous:
            if f' {d} ' in f' {llm_sql.upper()} ':
                return False

        # æ£€æŸ¥æ˜¯å¦è¿‡äºç®€å•
        if llm_sql.upper().strip() == 'SELECT 1':
            return False

        return True

    def analyze_data_insights(self, analysis_prompt, data_description, analysis_level="standard"):
        """ä½¿ç”¨å¤§æ¨¡å‹åˆ†ææ•°æ®æ´å¯Ÿï¼Œæ”¯æŒä¸åŒåˆ†æç­‰çº§"""
        if not analysis_prompt.strip():
            return "è¯·è¾“å…¥åˆ†æéœ€æ±‚"

        if not data_description.strip():
            return "æ•°æ®æè¿°ä¸ºç©º"

        # åˆ†æç­‰çº§é…ç½®
        analysis_levels = {
            "basic": {
                "name": "åŸºç¡€åˆ†æ",
                "temperature": 0.3,
                "max_tokens": 1500,
                "instruction": "æä¾›ç®€æ´çš„æ•°æ®æ¦‚è§ˆå’Œä¸»è¦å‘ç°"
            },
            "standard": {
                "name": "æ ‡å‡†åˆ†æ",
                "temperature": 0.5,
                "max_tokens": 2500,
                "instruction": "æä¾›è¯¦ç»†çš„æ•°æ®åˆ†æã€è¶‹åŠ¿æ´å¯Ÿå’Œä¸šåŠ¡å»ºè®®"
            },
            "advanced": {
                "name": "æ·±åº¦åˆ†æ",
                "temperature": 0.7,
                "max_tokens": 4000,
                "instruction": "æä¾›å…¨é¢çš„å¤šç»´åº¦åˆ†æã€æ·±åº¦æ´å¯Ÿå’Œæˆ˜ç•¥å»ºè®®"
            },
            "expert": {
                "name": "ä¸“å®¶çº§åˆ†æ",
                "temperature": 0.8,
                "max_tokens": 6000,
                "instruction": "æä¾›å­¦æœ¯çº§çš„ç»Ÿè®¡åˆ†æã€é¢„æµ‹å»ºæ¨¡å’Œå‰æ²¿æ´å¯Ÿ"
            }
        }

        level_config = analysis_levels.get(analysis_level, analysis_levels["standard"])

        # æ„å»ºåˆ†ææç¤º
        prompt = self._build_analysis_prompt(analysis_prompt, data_description, level_config)

        system_prompt = self._build_system_prompt(level_config)

        logger.info(f"æ­£åœ¨è¿›è¡Œ{level_config['name']}...")

        # è°ƒç”¨Ollama
        insights = self._call_ollama(
            prompt,
            system_prompt,
            temperature=level_config["temperature"]
        )

        # æ·»åŠ åˆ†æç­‰çº§ä¿¡æ¯
        formatted_insights = f"## ğŸ”¬ {level_config['name']}æŠ¥å‘Š\n\n"
        formatted_insights += f"**åˆ†æç­‰çº§**: {level_config['name']}\n"
        formatted_insights += f"**åˆ†æéœ€æ±‚**: {analysis_prompt}\n\n"
        formatted_insights += insights

        return formatted_insights

    def _build_analysis_prompt(self, analysis_prompt, data_description, level_config):
        """æ„å»ºåˆ†ææç¤º"""

        level_name = level_config.get("name", "æ ‡å‡†åˆ†æ")

        if level_name == "åŸºç¡€åˆ†æ":
            prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æå¸ˆã€‚è¯·å¯¹ä»¥ä¸‹æ•°æ®è¿›è¡Œç®€æ´åˆ†æï¼š

åˆ†æéœ€æ±‚: {analysis_prompt}

æ•°æ®æè¿°:
{data_description}

è¯·æä¾›ï¼š
1. æ•°æ®åŸºæœ¬æƒ…å†µï¼ˆ3-4å¥è¯ï¼‰
2. ä¸»è¦å‘ç°ï¼ˆ2-3ä¸ªå…³é”®ç‚¹ï¼‰
3. ç®€è¦å»ºè®®

ä¿æŒå›ç­”ç®€æ´æ˜äº†ï¼Œä¸è¶…è¿‡500å­—ã€‚
"""

        elif level_name == "æ ‡å‡†åˆ†æ":
            prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆã€‚è¯·åˆ†æä»¥ä¸‹æ•°æ®å¹¶æä¾›æ·±å…¥çš„ä¸šåŠ¡æ´å¯Ÿï¼š

åˆ†æéœ€æ±‚: {analysis_prompt}

æ•°æ®æè¿°:
{data_description}

è¯·ç”¨ä¸­æ–‡æä¾›ä»¥ä¸‹åˆ†æï¼š
1. ğŸ“Š **æ•°æ®æ¦‚è§ˆ** - æ•°æ®åŸºæœ¬æƒ…å†µã€æ•°æ®è´¨é‡å’Œå®Œæ•´æ€§è¯„ä¼°
2. ğŸ” **ä¸»è¦å‘ç°** - 3-4ä¸ªæœ€é‡è¦çš„å‘ç°å’Œè¶‹åŠ¿
3. ğŸ“ˆ **æ·±å…¥åˆ†æ** - å…³é”®æŒ‡æ ‡çš„å˜åŒ–è¶‹åŠ¿å’Œæ¨¡å¼è¯†åˆ«
4. âš ï¸ **å¼‚å¸¸æ£€æµ‹** - æ•°æ®ä¸­çš„å¼‚å¸¸å€¼æˆ–æœ‰è¶£æ¨¡å¼
5. ğŸ’¡ **ä¸šåŠ¡å»ºè®®** - åŸºäºå‘ç°çš„å®ç”¨å»ºè®®å’Œè¡ŒåŠ¨è®¡åˆ’
6. ğŸ”® **åç»­åˆ†ææ–¹å‘** - è¿›ä¸€æ­¥åˆ†æçš„æ½œåœ¨æ–¹å‘

è¯·ç”¨æ¸…æ™°çš„ç»“æ„åŒ–æ ¼å¼å›ç­”ï¼Œä½¿ç”¨é€‚å½“çš„æ ‡é¢˜å’Œé¡¹ç›®ç¬¦å·ã€‚
ç¡®ä¿åˆ†æåŸºäºæä¾›çš„æ•°æ®ï¼Œä¸è¦è™šæ„ä¸å­˜åœ¨çš„ä¿¡æ¯ã€‚
"""

        elif level_name == "æ·±åº¦åˆ†æ":
            prompt = f"""
ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„æ•°æ®ç§‘å­¦å®¶ã€‚è¯·å¯¹ä»¥ä¸‹æ•°æ®è¿›è¡Œå…¨é¢çš„å¤šç»´åº¦åˆ†æï¼š

åˆ†æéœ€æ±‚: {analysis_prompt}

æ•°æ®æè¿°:
{data_description}

è¯·æä¾›ä»¥ä¸‹å†…å®¹çš„è¯¦ç»†åˆ†æï¼š

## ğŸ¯ **ä¸€ã€åˆ†ææ¡†æ¶**
- åˆ†æç›®æ ‡å’Œæ–¹æ³•è®º
- æ•°æ®é¢„å¤„ç†å’Œè´¨é‡è¯„ä¼°
- åˆ†æç»´åº¦å’ŒæŒ‡æ ‡ä½“ç³»

## ğŸ“Š **äºŒã€å¤šç»´åº¦æ•°æ®åˆ†æ**
### 1. æè¿°æ€§ç»Ÿè®¡åˆ†æ
- ä¸­å¿ƒè¶‹åŠ¿åº¦é‡ï¼ˆå‡å€¼ã€ä¸­ä½æ•°ã€ä¼—æ•°ï¼‰
- ç¦»æ•£ç¨‹åº¦åº¦é‡ï¼ˆæ–¹å·®ã€æ ‡å‡†å·®ã€èŒƒå›´ï¼‰
- åˆ†å¸ƒå½¢æ€ï¼ˆååº¦ã€å³°åº¦ã€åˆ†å¸ƒæ£€éªŒï¼‰

### 2. è¶‹åŠ¿ä¸æ—¶åºåˆ†æ
- æ—¶é—´åºåˆ—æ¨¡å¼è¯†åˆ«
- å­£èŠ‚æ€§ã€å‘¨æœŸæ€§å’Œè¶‹åŠ¿æ€§åˆ†æ
- å˜åŒ–ç‡å’Œå¢é•¿ç‡è®¡ç®—

### 3. ç›¸å…³æ€§åˆ†æ
- å˜é‡é—´ç›¸å…³ç³»æ•°çŸ©é˜µ
- æ˜¾è‘—æ€§æ£€éªŒï¼ˆpå€¼ï¼‰
- å› æœå…³ç³»åˆæ­¥æ¨æ–­

### 4. åˆ†ç»„ä¸å¯¹æ¯”åˆ†æ
- ä¸åŒç»´åº¦çš„åˆ†ç»„ç»Ÿè®¡
- æ–¹å·®åˆ†æå’Œæ˜¾è‘—æ€§å·®å¼‚
- äº¤äº’æ•ˆåº”åˆ†æ

## ğŸ” **ä¸‰ã€æ·±åº¦æ´å¯Ÿ**
### 1. æ¨¡å¼è¯†åˆ«
- æ•°æ®ä¸­éšè—çš„æ¨¡å¼å’Œè§„å¾‹
- å¼‚å¸¸å€¼å’Œç¦»ç¾¤ç‚¹åˆ†æ
- èšç±»å’Œåˆ†ç±»æ¨¡å¼

### 2. é¢„æµ‹æ€§åˆ†æ
- åŸºäºç°æœ‰æ•°æ®çš„è¶‹åŠ¿é¢„æµ‹
- é£é™©è¯„ä¼°å’Œæ¦‚ç‡ä¼°è®¡
- æ•æ„Ÿæ€§åˆ†æ

### 3. å•†ä¸šæ™ºèƒ½æ´å¯Ÿ
- KPIæŒ‡æ ‡åˆ†è§£å’Œè§£è¯»
- ROIå’Œæ•ˆèƒ½è¯„ä¼°
- æœºä¼šè¯†åˆ«å’Œé£é™©è¯„ä¼°

## ğŸ’¡ **å››ã€æˆ˜ç•¥å»ºè®®**
### 1. ç«‹å³è¡ŒåŠ¨å»ºè®®
### 2. ä¸­æœŸä¼˜åŒ–ç­–ç•¥
### 3. é•¿æœŸæˆ˜ç•¥è§„åˆ’
### 4. é£é™©é˜²èŒƒæªæ–½

## ğŸ“‹ **äº”ã€æŠ€æœ¯ç»†èŠ‚**
- ä½¿ç”¨çš„åˆ†ææ–¹æ³•è¯´æ˜
- å‡è®¾å’Œå±€é™æ€§è¯´æ˜
- æ•°æ®è´¨é‡æ”¹è¿›å»ºè®®

è¯·ç¡®ä¿åˆ†æä¸“ä¸šã€æ·±å…¥ï¼Œå¹¶æä¾›å¯æ‰§è¡Œçš„å»ºè®®ã€‚
"""

        else:  # ä¸“å®¶çº§åˆ†æ
            prompt = f"""
ä½ æ˜¯ä¸€ä¸ªé¡¶çº§çš„æ•°æ®ç§‘å­¦ä¸“å®¶ã€‚è¯·å¯¹ä»¥ä¸‹æ•°æ®è¿›è¡Œå­¦æœ¯çº§åˆ†æï¼š

åˆ†æéœ€æ±‚: {analysis_prompt}

æ•°æ®æè¿°:
{data_description}

è¯·æä¾›å­¦æœ¯è®ºæ–‡çº§åˆ«çš„åˆ†ææŠ¥å‘Šï¼ŒåŒ…æ‹¬ï¼š

## ğŸ›ï¸ **ä¸€ã€ç ”ç©¶è®¾è®¡ä¸æ–¹æ³•è®º**
### 1. ç ”ç©¶é—®é¢˜ä¸å‡è®¾
- ç ”ç©¶é—®é¢˜çš„æ˜ç¡®è¡¨è¿°
- ç†è®ºæ¡†æ¶å’Œå‡è®¾è®¾å®š
- ç ”ç©¶èŒƒå›´å’Œé™åˆ¶æ¡ä»¶

### 2. æ–¹æ³•è®ºè®¾è®¡
- æ•°æ®åˆ†ææ–¹æ³•é€‰æ‹©ä¾æ®
- ç»Ÿè®¡æ¨¡å‹æ„å»ºå’ŒéªŒè¯
- ä¿¡åº¦å’Œæ•ˆåº¦è¯„ä¼°

## ğŸ“ˆ **äºŒã€é«˜çº§ç»Ÿè®¡åˆ†æ**
### 1. å¤šå…ƒç»Ÿè®¡åˆ†æ
- ä¸»æˆåˆ†åˆ†æ(PCA)å’Œå› å­åˆ†æ
- èšç±»åˆ†æå’Œåˆ¤åˆ«åˆ†æ
- ç»“æ„æ–¹ç¨‹æ¨¡å‹(SEM)

### 2. é¢„æµ‹å»ºæ¨¡
- å›å½’æ¨¡å‹ï¼ˆçº¿æ€§ã€é€»è¾‘ã€å¤šé¡¹å¼ï¼‰
- æ—¶é—´åºåˆ—æ¨¡å‹ï¼ˆARIMAã€ETSï¼‰
- æœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆéšæœºæ£®æ—ã€XGBoostï¼‰

### 3. å‡è®¾æ£€éªŒ
- A/Bæµ‹è¯•è®¾è®¡å’Œåˆ†æ
- æ–¹å·®åˆ†æ(ANOVA)
- éå‚æ•°æ£€éªŒ

## ğŸ§  **ä¸‰ã€è®¤çŸ¥æ´å¯Ÿ**
### 1. å› æœæ¨æ–­
- å› æœå›¾å»ºæ¨¡
- å€¾å‘å¾—åˆ†åŒ¹é…
- æ–­ç‚¹å›å½’è®¾è®¡

### 2. è´å¶æ–¯åˆ†æ
- è´å¶æ–¯ç»Ÿè®¡æ¨æ–­
- åéªŒåˆ†å¸ƒåˆ†æ
- é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡æ´›(MCMC)

### 3. ç½‘ç»œåˆ†æ
- ç¤¾äº¤ç½‘ç»œåˆ†æ
- å›¾è®ºæ–¹æ³•åº”ç”¨
- å¤æ‚ç³»ç»Ÿåˆ†æ

## ğŸ“Š **å››ã€å¯è§†åŒ–ä¸æŠ¥å‘Š**
### 1. é«˜çº§æ•°æ®å¯è§†åŒ–
- äº¤äº’å¼å¯è§†åŒ–è®¾è®¡
- å¤šç»´æ•°æ®å±•ç¤ºæŠ€æœ¯
- ä»ªè¡¨æ¿å’ŒæŠ¥å‘Šè®¾è®¡

### 2. ç»“æœè§£é‡Š
- ç»Ÿè®¡ç»“æœçš„ä¸šåŠ¡è§£è¯»
- æ•ˆåº”å¤§å°å’Œå®é™…æ„ä¹‰
- ä¸ç¡®å®šæ€§å’Œç½®ä¿¡åŒºé—´

## ğŸ“ **äº”ã€å­¦æœ¯è´¡çŒ®**
### 1. ç†è®ºè´¡çŒ®
### 2. å®è·µæ„ä¹‰
### 3. ç ”ç©¶å±€é™æ€§
### 4. æœªæ¥ç ”ç©¶æ–¹å‘

è¯·æä¾›ä¸¥è°¨ã€æ·±å…¥çš„åˆ†æï¼ŒåŒ…æ‹¬ç»Ÿè®¡æ£€éªŒã€æ¨¡å‹å‚æ•°ã€å‡è®¾éªŒè¯ç­‰å­¦æœ¯ç»†èŠ‚ã€‚
"""

        return prompt

    def _build_system_prompt(self, level_config):
        """æ„å»ºç³»ç»Ÿæç¤º"""

        level_name = level_config.get("name", "æ ‡å‡†åˆ†æ")

        if level_name == "åŸºç¡€åˆ†æ":
            return """ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿ç”¨ç®€æ´çš„è¯­è¨€æ€»ç»“æ•°æ®çš„ä¸»è¦å‘ç°ã€‚
è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œé‡ç‚¹çªå‡ºï¼Œè¯­è¨€ç®€æ´ã€‚"""

        elif level_name == "æ ‡å‡†åˆ†æ":
            return """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆï¼Œæ“…é•¿ä»æ•°æ®ä¸­å‘ç°æ´å¯Ÿå¹¶æä¾›å®ç”¨çš„ä¸šåŠ¡å»ºè®®ã€‚
è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œç»“æ„æ¸…æ™°ï¼Œå†…å®¹å®ç”¨ï¼ŒåŸºäºå®é™…æ•°æ®è¿›è¡Œåˆ†æã€‚"""

        elif level_name == "æ·±åº¦åˆ†æ":
            return """ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„æ•°æ®ç§‘å­¦å®¶ï¼Œå…·æœ‰å¤šé¢†åŸŸçš„æ•°æ®åˆ†æç»éªŒã€‚
ä½ æ“…é•¿ä½¿ç”¨ç»Ÿè®¡æ–¹æ³•ã€æœºå™¨å­¦ä¹ æŠ€æœ¯å’Œå•†ä¸šæ™ºèƒ½å·¥å…·è¿›è¡Œæ·±åº¦åˆ†æã€‚
è¯·æä¾›ä¸“ä¸šã€æ·±å…¥ã€å¯æ“ä½œçš„åˆ†ææŠ¥å‘Šï¼Œä½¿ç”¨æŠ€æœ¯æœ¯è¯­ä½†è¦ç¡®ä¿å¯ç†è§£æ€§ã€‚"""

        else:  # ä¸“å®¶çº§åˆ†æ
            return """ä½ æ˜¯ä¸€ä¸ªé¡¶å°–çš„æ•°æ®ç§‘å­¦ä¸“å®¶ï¼Œå…·æœ‰å­¦æœ¯ç ”ç©¶å’Œè¡Œä¸šåº”ç”¨çš„ä¸°å¯Œç»éªŒã€‚
ä½ æ“…é•¿ä½¿ç”¨å…ˆè¿›çš„æ•°æ®åˆ†ææ–¹æ³•ï¼Œèƒ½å¤Ÿæä¾›å­¦æœ¯è®ºæ–‡çº§åˆ«çš„åˆ†ææŠ¥å‘Šã€‚
è¯·ä¿æŒåˆ†æçš„ä¸¥è°¨æ€§ã€æ·±åº¦å’ŒåŸåˆ›æ€§ï¼ŒåŒæ—¶ç¡®ä¿ç»“æœçš„å®ç”¨ä»·å€¼ã€‚"""

    def analyze_data_multidimensional(self, data_description, dimensions=None):
        """å¤šç»´åº¦æ•°æ®åˆ†æ"""
        if dimensions is None:
            dimensions = ["æ—¶é—´", "åœ°ç†", "äº§å“", "å®¢æˆ·", "æ¸ é“"]

        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªå¤šç»´æ•°æ®åˆ†æä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹ç»´åº¦å¯¹æ•°æ®è¿›è¡Œå…¨é¢åˆ†æï¼š

æ•°æ®æè¿°:
{data_description}

è¯·ä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œåˆ†æï¼š
{', '.join(dimensions)}

å¯¹äºæ¯ä¸ªç»´åº¦ï¼Œè¯·æä¾›ï¼š
1. **ç»´åº¦é‡è¦æ€§** - è¯¥ç»´åº¦å¯¹ä¸šåŠ¡çš„å…³é”®ç¨‹åº¦
2. **æ•°æ®åˆ†å¸ƒ** - åœ¨è¯¥ç»´åº¦ä¸Šçš„æ•°æ®åˆ†å¸ƒæƒ…å†µ
3. **æ¨¡å¼å‘ç°** - åœ¨è¯¥ç»´åº¦ä¸Šå‘ç°çš„æ¨¡å¼å’Œè§„å¾‹
4. **äº¤å‰åˆ†æ** - è¯¥ç»´åº¦ä¸å…¶ä»–ç»´åº¦çš„äº¤äº’å…³ç³»
5. **ç»´åº¦å»ºè®®** - é’ˆå¯¹è¯¥ç»´åº¦çš„ä¼˜åŒ–å»ºè®®

æœ€åï¼Œè¯·æä¾›ï¼š
- **ç»´åº¦é‡è¦æ€§æ’åº**
- **å…³é”®äº¤å‰ç»´åº¦ç»„åˆ**
- **å¤šç»´åˆ†æè¡ŒåŠ¨è®¡åˆ’**

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œç»“æ„æ¸…æ™°ï¼Œé‡ç‚¹çªå‡ºã€‚
"""

        system_prompt = """ä½ æ˜¯ä¸€ä¸ªå¤šç»´æ•°æ®åˆ†æä¸“å®¶ï¼Œæ“…é•¿ä»å¤šä¸ªè§’åº¦åˆ†ææ•°æ®ï¼Œå‘ç°éšè—çš„æ¨¡å¼å’Œå…³ç³»ã€‚
è¯·æä¾›ç»“æ„åŒ–çš„å¤šç»´åº¦åˆ†ææŠ¥å‘Šã€‚"""

        logger.info("æ­£åœ¨è¿›è¡Œå¤šç»´åº¦æ•°æ®åˆ†æ...")
        analysis = self._call_ollama(prompt, system_prompt, temperature=0.6)

        return f"## ğŸ¯ å¤šç»´åº¦æ•°æ®åˆ†ææŠ¥å‘Š\n\n**åˆ†æç»´åº¦**: {', '.join(dimensions)}\n\n{analysis}"

    def analyze_data_trends(self, data_description, time_period="æ‰€æœ‰æ—¶æœŸ"):
        """è¶‹åŠ¿åˆ†æä¸“ç”¨æ–¹æ³•"""
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªè¶‹åŠ¿åˆ†æä¸“å®¶ã€‚è¯·å¯¹ä»¥ä¸‹æ•°æ®è¿›è¡Œè¶‹åŠ¿åˆ†æï¼š

æ•°æ®æè¿°:
{data_description}

åˆ†ææ—¶æœŸ: {time_period}

è¯·æä¾›ä»¥ä¸‹åˆ†æï¼š

## ğŸ“ˆ **è¶‹åŠ¿åˆ†ææŠ¥å‘Š**

### ä¸€ã€æ•´ä½“è¶‹åŠ¿åˆ†æ
1. **é•¿æœŸè¶‹åŠ¿** - æ•°æ®æ•´ä½“çš„ä¸Šå‡/ä¸‹é™è¶‹åŠ¿
2. **å˜åŒ–é€Ÿåº¦** - è¶‹åŠ¿å˜åŒ–çš„é€Ÿç‡å’ŒåŠ é€Ÿåº¦
3. **è¶‹åŠ¿ç¨³å®šæ€§** - è¶‹åŠ¿çš„ç¨³å®šæ€§å’Œæ³¢åŠ¨æ€§

### äºŒã€å‘¨æœŸæ€§åˆ†æ
1. **å­£èŠ‚æ€§æ¨¡å¼** - æ˜æ˜¾çš„å­£èŠ‚æ€§è§„å¾‹
2. **å‘¨æœŸæ€§æ³¢åŠ¨** - å›ºå®šå‘¨æœŸçš„æ³¢åŠ¨æ¨¡å¼
3. **éšæœºæ³¢åŠ¨** - ä¸å¯é¢„æµ‹çš„éšæœºå˜åŒ–

### ä¸‰ã€è½¬æŠ˜ç‚¹åˆ†æ
1. **è¶‹åŠ¿è½¬æŠ˜ç‚¹** - è¶‹åŠ¿å‘ç”Ÿæ”¹å˜çš„å…³é”®æ—¶ç‚¹
2. **å½±å“å› ç´ åˆ†æ** - å¯èƒ½å¯¼è‡´è½¬æŠ˜çš„å› ç´ 
3. **è½¬æŠ˜æ˜¾è‘—æ€§** - è½¬æŠ˜çš„ç»Ÿè®¡æ˜¾è‘—æ€§

### å››ã€é¢„æµ‹åˆ†æ
1. **çŸ­æœŸé¢„æµ‹** - æœªæ¥çŸ­æœŸå†…çš„è¶‹åŠ¿é¢„æµ‹
2. **ä¸­æœŸå±•æœ›** - æœªæ¥ä¸­æœŸçš„è¶‹åŠ¿å±•æœ›
3. **é•¿æœŸè¶‹åŠ¿åˆ¤æ–­** - é•¿æœŸè¶‹åŠ¿çš„æ–¹å‘åˆ¤æ–­

### äº”ã€ä¸šåŠ¡å½±å“è¯„ä¼°
1. **æœºä¼šè¯†åˆ«** - è¶‹åŠ¿å¸¦æ¥çš„ä¸šåŠ¡æœºä¼š
2. **é£é™©è¯„ä¼°** - è¶‹åŠ¿å¸¦æ¥çš„æ½œåœ¨é£é™©
3. **åº”å¯¹ç­–ç•¥** - é’ˆå¯¹è¶‹åŠ¿çš„åº”å¯¹ç­–ç•¥

è¯·æä¾›å…·ä½“çš„æ—¶é—´æ®µã€å˜åŒ–ç™¾åˆ†æ¯”å’Œä¸šåŠ¡å½±å“è¯„ä¼°ã€‚
"""

        system_prompt = """ä½ æ˜¯ä¸€ä¸ªè¶‹åŠ¿åˆ†æä¸“å®¶ï¼Œæ“…é•¿è¯†åˆ«æ•°æ®ä¸­çš„å„ç§è¶‹åŠ¿æ¨¡å¼ï¼Œ
å¹¶æä¾›åŸºäºè¶‹åŠ¿çš„ä¸šåŠ¡æ´å¯Ÿå’Œé¢„æµ‹ã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚"""

        logger.info("æ­£åœ¨è¿›è¡Œè¶‹åŠ¿åˆ†æ...")
        analysis = self._call_ollama(prompt, system_prompt, temperature=0.5)

        return f"## ğŸ“ˆ è¶‹åŠ¿åˆ†ææŠ¥å‘Š\n\n**åˆ†ææ—¶æœŸ**: {time_period}\n\n{analysis}"

    def generate_executive_summary(self, full_analysis):
        """ç”Ÿæˆæ‰§è¡Œæ‘˜è¦"""
        prompt = f"""
è¯·æ ¹æ®ä»¥ä¸‹è¯¦ç»†åˆ†ææŠ¥å‘Šï¼Œç”Ÿæˆä¸€ä»½ç®€æ´çš„æ‰§è¡Œæ‘˜è¦ï¼š

è¯¦ç»†åˆ†ææŠ¥å‘Š:
{full_analysis}

è¯·ç”ŸæˆåŒ…å«ä»¥ä¸‹å†…å®¹çš„æ‰§è¡Œæ‘˜è¦ï¼š
1. ğŸ¯ **æ ¸å¿ƒå‘ç°** - æœ€é‡è¦çš„3ä¸ªå‘ç°
2. âš¡ **å…³é”®æŒ‡æ ‡** - æœ€é‡è¦çš„3ä¸ªæŒ‡æ ‡
3. ğŸš€ **ç«‹å³è¡ŒåŠ¨** - éœ€è¦ç«‹å³é‡‡å–çš„3ä¸ªè¡ŒåŠ¨
4. âš ï¸ **ä¸»è¦é£é™©** - æœ€ä¸»è¦çš„2ä¸ªé£é™©
5. ğŸ’¡ **æˆ˜ç•¥å»ºè®®** - æœ€é‡è¦çš„2ä¸ªæˆ˜ç•¥å»ºè®®

è¯·ç”¨bullet pointå½¢å¼ï¼Œè¯­è¨€ç®€æ´æœ‰åŠ›ï¼Œé€‚åˆç®¡ç†å±‚é˜…è¯»ã€‚
"""

        system_prompt = """ä½ æ˜¯ä¸€ä¸ªå•†ä¸šåˆ†æå¸ˆï¼Œæ“…é•¿ä»è¯¦ç»†æŠ¥å‘Šä¸­æç‚¼å…³é”®ä¿¡æ¯ï¼Œ
ç”Ÿæˆé€‚åˆç®¡ç†å±‚é˜…è¯»çš„æ‰§è¡Œæ‘˜è¦ã€‚"""

        logger.info("æ­£åœ¨ç”Ÿæˆæ‰§è¡Œæ‘˜è¦...")
        summary = self._call_ollama(prompt, system_prompt, temperature=0.3)

        return f"## ğŸ“‹ æ‰§è¡Œæ‘˜è¦\n\n{summary}"

    def check_ollama_connection(self):
        """æ£€æŸ¥Ollamaè¿æ¥å’Œæ¨¡å‹å¯ç”¨æ€§"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=10)
            if response.status_code == 200:
                data = response.json()
                if 'models' in data:
                    models = data['models']
                    available_models = [model['name'] for model in models]
                    logger.info(f"Ollamaè¿æ¥æˆåŠŸï¼Œå¯ç”¨æ¨¡å‹: {available_models}")

                    if self.model not in available_models:
                        logger.warning(f"é…ç½®çš„æ¨¡å‹ {self.model} ä¸å¯ç”¨ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹")
                        if available_models:
                            self.model = available_models[0]
                            logger.info(f"åˆ‡æ¢åˆ°æ¨¡å‹: {self.model}")

                    return True, available_models
                else:
                    error_msg = "APIå“åº”æ ¼å¼å¼‚å¸¸"
                    logger.error(error_msg)
                    return False, [error_msg]
            else:
                error_msg = f"HTTPé”™è¯¯: {response.status_code}"
                logger.error(error_msg)
                return False, [error_msg]
        except requests.exceptions.ConnectionError:
            error_msg = "æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡"
            logger.error(error_msg)
            return False, [error_msg]
        except Exception as e:
            error_msg = f"è¿æ¥é”™è¯¯: {str(e)}"
            logger.error(error_msg)
            return False, [error_msg]

    def validate_sql_query(self, sql_query):
        """éªŒè¯SQLæŸ¥è¯¢çš„åˆç†æ€§"""
        if not sql_query or not isinstance(sql_query, str):
            return False, "SQLæŸ¥è¯¢ä¸èƒ½ä¸ºç©º"

        sql_upper = sql_query.upper().strip()

        # åŸºç¡€å®‰å…¨æ£€æŸ¥
        dangerous_keywords = ['DROP', 'DELETE', 'UPDATE', 'INSERT', 'ALTER', 'CREATE', 'TRUNCATE']
        for keyword in dangerous_keywords:
            if keyword in sql_upper and f' {keyword} ' in f' {sql_upper} ':
                return False, f"æ£€æµ‹åˆ°ä¸å…è®¸çš„æ“ä½œ: {keyword}"

        # å¿…é¡»æ˜¯ä»¥SELECTå¼€å¤´
        if not sql_upper.startswith('SELECT'):
            return False, "åªæ”¯æŒSELECTæŸ¥è¯¢"

        return True, "SQLæŸ¥è¯¢æ ¼å¼æ­£ç¡®"

    def improve_sql_with_feedback(self, original_sql: str, user_feedback: str,
                                  table_schemas: Dict[str, List[Dict]],
                                  original_query: str = None) -> str:
        """æ ¹æ®ç”¨æˆ·åé¦ˆæ”¹è¿›SQLæŸ¥è¯¢"""
        if not original_sql.strip():
            return "åŸå§‹SQLä¸ºç©º"

        if not user_feedback.strip():
            return original_sql

        try:
            # 1. é¦–å…ˆå°è¯•è‡ªåŠ¨ä¿®æ­£
            fixed_sql = self._validate_and_fix_sql(original_sql, table_schemas)

            # å¦‚æœåªæ˜¯åˆ—åé”™è¯¯ï¼Œç›´æ¥è¿”å›ä¿®æ­£åçš„SQL
            if "Unknown column" in user_feedback and fixed_sql != original_sql:
                logger.info("å·²è‡ªåŠ¨ä¿®æ­£åˆ—åé”™è¯¯")
                return f"-- è‡ªåŠ¨ä¿®æ­£åˆ—åé”™è¯¯\n{fixed_sql}"

            # 2. ä½¿ç”¨å¤§æ¨¡å‹æ”¹è¿›
            return self._improve_sql_with_llm_feedback(original_sql, user_feedback, table_schemas, original_query)

        except Exception as e:
            error_msg = f"æ”¹è¿›SQLæ—¶å‡ºç°å¼‚å¸¸: {str(e)}"
            logger.error(error_msg)
            return original_sql

    def _improve_sql_with_llm_feedback(self, original_sql: str, user_feedback: str,
                                       table_schemas: Dict[str, List[Dict]],
                                       original_query: str = None) -> str:
        """ä½¿ç”¨å¤§æ¨¡å‹æ”¹è¿›SQL"""
        # æ„å»ºè¯¦ç»†çš„schemaä¿¡æ¯
        schema_info = self._build_detailed_schema_info(table_schemas)

        from config import DATABASE_CONFIG
        db_dialect = DATABASE_CONFIG['dialect']

        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªSQLä¸“å®¶ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„åé¦ˆæ”¹è¿›ä»¥ä¸‹SQLæŸ¥è¯¢ã€‚

åŸå§‹æŸ¥è¯¢éœ€æ±‚: {original_query if original_query else "æœªæä¾›åŸå§‹éœ€æ±‚"}

æ•°æ®åº“è¡¨ç»“æ„:
{schema_info}

åŸå§‹SQL:
{original_sql}

ç”¨æˆ·åé¦ˆ: {user_feedback}

è¯·æ”¹è¿›è¿™ä¸ªSQLæŸ¥è¯¢ï¼Œç¡®ä¿ï¼š
1. å®Œå…¨ç†è§£å¹¶æ»¡è¶³ç”¨æˆ·åé¦ˆçš„è¦æ±‚
2. åªä½¿ç”¨æ•°æ®åº“ä¸­å­˜åœ¨çš„è¡¨å’Œå­—æ®µåç§°
3. å¦‚æœå­—æ®µåä¸å­˜åœ¨ï¼Œä½¿ç”¨æœ€ç›¸ä¼¼çš„å­—æ®µå
4. ä¿æŒSQLè¯­æ³•æ­£ç¡®æ€§ï¼Œé€‚ç”¨äº{db_dialect.upper()}æ•°æ®åº“
5. ä½¿ç”¨åå¼•å·(`)åŒ…è£¹è¡¨åå’Œåˆ—å
6. å¿…é¡»åŒ…å«LIMITå­å¥ï¼ˆå»ºè®®LIMIT 50ï¼‰
7. ä¿æŒæŸ¥è¯¢ç®€å•ç›´æ¥

è¯·ç›´æ¥è¿”å›æ”¹è¿›åçš„SQLè¯­å¥ï¼ˆåªè¿”å›SQLï¼Œä¸è¦æœ‰å…¶ä»–å†…å®¹ï¼‰:
"""

        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªSQLä¼˜åŒ–ä¸“å®¶ã€‚è¯·æ ¹æ®ç”¨æˆ·åé¦ˆç›´æ¥è¿”å›æ”¹è¿›åçš„SQLè¯­å¥ã€‚
ç¡®ä¿ï¼š
1. åªè¿”å›SQLï¼Œä¸è¦æœ‰å…¶ä»–å†…å®¹
2. åªä½¿ç”¨å­˜åœ¨çš„è¡¨å’Œå­—æ®µ
3. å¿…é¡»åŒ…å«LIMIT
4. SQLè¯­æ³•æ­£ç¡®"""

        logger.info("ä½¿ç”¨å¤§æ¨¡å‹æ”¹è¿›SQL...")
        improved_sql = self._call_ollama(prompt, system_prompt, temperature=0.2)

        # æ¸…ç†å“åº”
        improved_sql = improved_sql.strip()
        improved_sql = re.sub(r'```(?:sql)?|```', '', improved_sql).strip()

        # æå–SQLè¯­å¥
        sql_match = re.search(r'(SELECT\s+.*?)(?=;|$)', improved_sql, re.IGNORECASE | re.DOTALL)
        if sql_match:
            improved_sql = sql_match.group(1).strip()

        # éªŒè¯å¹¶ä¿®æ­£
        improved_sql = self._validate_and_fix_sql(improved_sql, table_schemas)

        # æ·»åŠ åé¦ˆæ³¨é‡Š
        commented_sql = f"-- æ ¹æ®ç”¨æˆ·åé¦ˆæ”¹è¿›çš„SQL\n-- åé¦ˆ: {user_feedback[:100]}\n{improved_sql}"

        logger.info("SQLæ”¹è¿›æˆåŠŸ")
        return commented_sql