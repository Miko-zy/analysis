# analysis.py
import pandas as pd
import numpy as np
import warnings
import logging
import traceback
from typing import Optional, Dict, List, Tuple, Any, Set
import math
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

COLOR_PALETTES = {
    'categorical': ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',
                    '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'],
    'sequential': ['#f7fbff', '#deebf7', '#c6dbef', '#9ecae1', '#6baed6',
                   '#4292c6', '#2171b5', '#08519c', '#08306b'],
    'diverging': ['#67001f', '#b2182b', '#d6604d', '#f4a582', '#fddbc7',
                  '#f7f7f7', '#d1e5f0', '#92c5de', '#4393c3', '#2166ac', '#053061']
}

SCIENTIFIC_CONFIG = {
    'dpi': 100,
    'font_size': 10,
    'title_size': 14,
    'label_size': 11,
    'tick_size': 9,
    'line_width': 2,
    'marker_size': 50,
    'grid_alpha': 0.3,
    'figure_ratio': 1.618,
    'max_categories': 10,
}

CHART_CONFIGS = {
    'line': {
        'title': 'è¶‹åŠ¿åˆ†æå›¾',
        'description': 'å±•ç¤ºå˜é‡éšæ—¶é—´æˆ–å…¶ä»–è¿ç»­å˜é‡çš„å˜åŒ–è¶‹åŠ¿',
        'requirements': {
            'x_type': ['datetime', 'numeric', 'ordinal'],
            'y_type': ['numeric'],
            'x_role': 'è‡ªå˜é‡ï¼ˆæ—¶é—´/åºåˆ—ï¼‰',
            'y_role': 'å› å˜é‡ï¼ˆæ•°å€¼æŒ‡æ ‡ï¼‰'
        },
        'best_for': ['æ—¶é—´åºåˆ—', 'è¶‹åŠ¿åˆ†æ', 'å‘¨æœŸæ€§å˜åŒ–'],
        'logic': 'å±•ç¤ºYå˜é‡å¦‚ä½•éšXå˜é‡å˜åŒ–ï¼Œé€‚åˆçœ‹è¶‹åŠ¿'
    },
    'bar': {
        'title': 'æ¯”è¾ƒåˆ†æå›¾',
        'description': 'ç”¨äºæ¯”è¾ƒä¸åŒç±»åˆ«ä¹‹é—´çš„æ•°å€¼å·®å¼‚',
        'requirements': {
            'x_type': ['categorical', 'ordinal'],
            'y_type': ['numeric'],
            'x_role': 'åˆ†ç±»å˜é‡',
            'y_role': 'æ•°å€¼æŒ‡æ ‡'
        },
        'best_for': ['åˆ†ç±»æ¯”è¾ƒ', 'æ’ååˆ†æ', 'å æ¯”åˆ†æ'],
        'logic': 'é€šè¿‡æŸ±å­é«˜åº¦æ¯”è¾ƒä¸åŒç±»åˆ«çš„æ•°å€¼å¤§å°'
    },
    'scatter': {
        'title': 'ç›¸å…³æ€§åˆ†æå›¾',
        'description': 'å±•ç¤ºä¸¤ä¸ªæ•°å€¼å˜é‡ä¹‹é—´çš„ç›¸å…³å…³ç³»',
        'requirements': {
            'x_type': ['numeric'],
            'y_type': ['numeric'],
            'x_role': 'è‡ªå˜é‡',
            'y_role': 'å› å˜é‡'
        },
        'best_for': ['ç›¸å…³æ€§åˆ†æ', 'èšç±»åˆ†æ', 'å¼‚å¸¸æ£€æµ‹'],
        'logic': 'é€šè¿‡ç‚¹çš„åˆ†å¸ƒè§‚å¯Ÿä¸¤ä¸ªå˜é‡é—´çš„å…³ç³»'
    },
    'histogram': {
        'title': 'åˆ†å¸ƒåˆ†æå›¾',
        'description': 'å±•ç¤ºå•ä¸€æ•°å€¼å˜é‡çš„åˆ†å¸ƒæƒ…å†µ',
        'requirements': {
            'x_type': ['numeric'],
            'y_type': [],  # è‡ªåŠ¨ç”Ÿæˆé¢‘ç‡
            'x_role': 'æ•°å€¼å˜é‡',
            'y_role': 'é¢‘æ¬¡/å¯†åº¦'
        },
        'best_for': ['åˆ†å¸ƒåˆ†æ', 'å¼‚å¸¸å€¼æ£€æµ‹', 'æ•°æ®è´¨é‡æ£€æŸ¥'],
        'logic': 'é€šè¿‡æŸ±å­é«˜åº¦è¡¨ç¤ºæ•°å€¼è½åœ¨å„åŒºé—´çš„é¢‘æ¬¡'
    },
    'pie': {
        'title': 'æ„æˆåˆ†æå›¾',
        'description': 'å±•ç¤ºå„éƒ¨åˆ†å æ€»ä½“çš„æ¯”ä¾‹å…³ç³»',
        'requirements': {
            'x_type': ['categorical'],
            'y_type': ['numeric'],
            'x_role': 'ç»„æˆéƒ¨åˆ†',
            'y_role': 'æ•°å€¼æŒ‡æ ‡'
        },
        'best_for': ['å æ¯”åˆ†æ', 'æ„æˆåˆ†æ'],
        'logic': 'é€šè¿‡æ‰‡å½¢é¢ç§¯è¡¨ç¤ºå„éƒ¨åˆ†å æ¯”'
    },
    'box': {
        'title': 'ç®±çº¿å›¾',
        'description': 'å±•ç¤ºæ•°æ®çš„åˆ†å¸ƒç‰¹å¾å’Œå¼‚å¸¸å€¼',
        'requirements': {
            'x_type': ['categorical', 'numeric'],
            'y_type': ['numeric'],
            'x_role': 'åˆ†ç»„å˜é‡',
            'y_role': 'æ•°å€¼å˜é‡'
        },
        'best_for': ['åˆ†å¸ƒæ¯”è¾ƒ', 'å¼‚å¸¸å€¼æ£€æµ‹', 'ç¦»æ•£ç¨‹åº¦åˆ†æ'],
        'logic': 'é€šè¿‡ç®±å­å’Œé¡»çº¿å±•ç¤ºæ•°æ®çš„å››åˆ†ä½æ•°å’Œå¼‚å¸¸å€¼'
    }
}


class DataAnalyzer:
    def __init__(self):
        """åˆå§‹åŒ–æ•°æ®åˆ†æå™¨"""
        self.current_figure = None
        self.field_analysis_cache = {}
        logger.info("ğŸš€ æ™ºèƒ½æ•°æ®åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")

    def _analyze_column(self, series: pd.Series) -> Dict[str, Any]:
        """æ·±å…¥åˆ†æå•ä¸ªå­—æ®µ"""
        col_name = series.name if hasattr(series, 'name') else 'unknown'

        # åŸºæœ¬ç±»å‹æ£€æµ‹
        if pd.api.types.is_numeric_dtype(series):
            col_type = 'numeric'
        elif pd.api.types.is_datetime64_any_dtype(series):
            col_type = 'datetime'
        elif pd.api.types.is_bool_dtype(series):
            col_type = 'boolean'
        else:
            unique_count = series.nunique()
            total_count = len(series)
            unique_ratio = unique_count / total_count if total_count > 0 else 0

            if unique_count <= 10 or unique_ratio < 0.1:
                col_type = 'categorical'
            elif unique_count <= 50:
                col_type = 'ordinal'
            else:
                col_type = 'text'

        analysis = {
            'name': col_name,
            'type': col_type,
            'dtype': str(series.dtype),
            'unique_count': series.nunique(),
            'null_count': series.isnull().sum(),
            'null_percentage': series.isnull().mean() * 100,
        }

        # æ•°å€¼å‹å­—æ®µçš„è¯¦ç»†åˆ†æ
        if col_type == 'numeric':
            analysis.update({
                'min': series.min(),
                'max': series.max(),
                'mean': series.mean(),
                'median': series.median(),
                'std': series.std(),
                'skewness': series.skew(),
                'kurtosis': series.kurtosis(),
                'range': series.max() - series.min(),
                'iqr': series.quantile(0.75) - series.quantile(0.25),
                'is_percentage': any(x in col_name.lower() for x in ['rate', 'ratio', 'percent', '%']),
                'is_amount': any(x in col_name.lower() for x in ['amount', 'price', 'cost', 'revenue', 'sales']),
                'is_count': any(x in col_name.lower() for x in ['count', 'number', 'quantity', 'qty']),
                'is_id': any(x in col_name.lower() for x in ['id', 'code', 'no', 'num']),
            })

            # åˆ¤æ–­æ˜¯å¦å¯èƒ½æ˜¯æ—¶é—´åºåˆ—
            if series.dropna().between(1900, 2100).all():
                analysis['potential_time'] = True
                analysis['time_unit'] = 'year'
            elif series.dropna().between(1, 12).all():
                analysis['potential_time'] = True
                analysis['time_unit'] = 'month'

        # æ–‡æœ¬å‹å­—æ®µçš„è¯¦ç»†åˆ†æ
        elif col_type == 'text':
            lengths = series.dropna().astype(str).str.len()
            analysis.update({
                'avg_length': lengths.mean(),
                'min_length': lengths.min(),
                'max_length': lengths.max(),
                'common_prefix': self._find_common_prefix(series),
            })

        # åˆ†ç±»å­—æ®µçš„è¯¦ç»†åˆ†æ
        elif col_type in ['categorical', 'ordinal']:
            value_counts = series.value_counts()
            analysis.update({
                'top_categories': value_counts.head(5).to_dict(),
                'category_distribution': value_counts.to_dict(),
                'category_count': len(value_counts),
            })

        return analysis

    def _find_common_prefix(self, series: pd.Series) -> Optional[str]:
        """æŸ¥æ‰¾å­—ç¬¦ä¸²çš„å…¬å…±å‰ç¼€"""
        non_null_values = series.dropna().astype(str)
        if len(non_null_values) < 2:
            return None

        prefix = non_null_values.iloc[0]
        for value in non_null_values.iloc[1:]:
            while not value.startswith(prefix) and prefix:
                prefix = prefix[:-1]
            if not prefix:
                break
        return prefix if len(prefix) > 1 else None

    def generate_summary_statistics(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ç”Ÿæˆæ•°æ®æ‘˜è¦ç»Ÿè®¡"""
        if df.empty:
            return {"error": "æ•°æ®ä¸ºç©º"}

        summary = {
            'basic_info': {
                'total_rows': len(df),
                'total_columns': len(df.columns),
                'memory_usage': df.memory_usage(deep=True).sum()
            },
            'columns': {}
        }

        # åˆ†ææ¯ä¸€åˆ—
        for column in df.columns:
            try:
                analysis = self._analyze_column(df[column])
                summary['columns'][column] = analysis
            except Exception as e:
                logger.error(f"åˆ†æåˆ— {column} æ—¶å‡ºé”™: {e}")
                summary['columns'][column] = {'error': str(e)}

        return summary

    def create_visualization(self, df: pd.DataFrame, chart_type: str, 
                           x_column: str, y_column: Optional[str] = None,
                           group_by: Optional[str] = None) -> Dict[str, Any]:
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨è§„èŒƒ"""
        try:
            if df.empty:
                return {"error": "æ•°æ®ä¸ºç©º"}

            # æ£€æŸ¥æ‰€éœ€åˆ—æ˜¯å¦å­˜åœ¨
            required_columns = [x_column]
            if y_column:
                required_columns.append(y_column)
            if group_by:
                required_columns.append(group_by)

            missing_columns = [col for col in required_columns if col not in df.columns]
            if missing_columns:
                return {"error": f"ç¼ºå°‘åˆ—: {missing_columns}"}

            # æ„å»ºå¯è§†åŒ–è§„èŒƒ
            viz_spec = {
                'chart_type': chart_type,
                'x_column': x_column,
                'y_column': y_column,
                'group_by': group_by,
                'data_sample': df.head(100).to_dict('records')  # é™åˆ¶æ ·æœ¬å¤§å°
            }

            return {"success": True, "spec": viz_spec}

        except Exception as e:
            logger.error(f"åˆ›å»ºå¯è§†åŒ–å¤±è´¥: {e}")
            return {"error": str(e)}